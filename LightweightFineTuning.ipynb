{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "* PEFT technique:LoRA\n",
        "* Model:bert-base-uncased\n",
        "* Evaluation approach:epoch\n",
        "* Fine-tuning dataset: imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f551c63a",
      "metadata": {
        "id": "f551c63a",
        "outputId": "3ecf36e9-18fc-4326-a549-21e8bce2289f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4935cb4d",
      "metadata": {
        "id": "4935cb4d",
        "outputId": "5311cddd-a501-4618-afaa-3fa1e2fef6f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:39]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Accuracy: 0.471\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import numpy as np\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "base_training_args = TrainingArguments(\n",
        "        output_dir=\"/tmp/BertFineTuningClassifier/base_results\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "base_trainer = Trainer(\n",
        "    model = model,\n",
        "    args = base_training_args,\n",
        "    train_dataset= tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "base_results = base_trainer.evaluate()\n",
        "print(\"Base Accuracy:\", base_results[\"eval_accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28c4a78",
      "metadata": {
        "id": "f28c4a78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019b9f55",
      "metadata": {
        "id": "019b9f55"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5176b07f",
      "metadata": {
        "id": "5176b07f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5775fadf",
      "metadata": {
        "id": "5775fadf"
      },
      "outputs": [],
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894046c0",
      "metadata": {
        "id": "894046c0",
        "outputId": "41ff6745-1819-498b-d1d0-884c138bbf0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 297,988 || all params: 109,780,228 || trainable%: 0.27144050019644705\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha = 16,\n",
        "    target_modules = [\"query\", \"value\"],\n",
        "    lora_dropout = 0.1,\n",
        "    bias = \"none\",\n",
        "    task_type = \"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model,lora_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d4c908",
      "metadata": {
        "id": "c4d4c908",
        "outputId": "f28a8619-6c35-4169-8b9b-2bc4a04c83a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 09:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.700700</td>\n",
              "      <td>0.652537</td>\n",
              "      <td>0.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.520500</td>\n",
              "      <td>0.527727</td>\n",
              "      <td>0.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.479475</td>\n",
              "      <td>0.787000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.5858481879234314, metrics={'train_runtime': 597.9129, 'train_samples_per_second': 10.035, 'train_steps_per_second': 2.509, 'total_flos': 1584130498560000.0, 'train_loss': 0.5858481879234314, 'epoch': 3.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import numpy as np\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"/tmp/BertFineTuningClassifier/tuning_results\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = lora_model,\n",
        "    args = training_args,\n",
        "    train_dataset= tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47abf88",
      "metadata": {
        "id": "b47abf88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7fe003",
      "metadata": {
        "id": "fa7fe003"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "lora_model.save_pretrained(\"/tmp/LightTunedLoraModel_Final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863ec66e",
      "metadata": {
        "id": "863ec66e",
        "outputId": "0491bece-e7ad-4e1b-9a7c-be17f42f4f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['adapter_model.bin', 'adapter_config.json', 'README.md']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path where you saved the model\n",
        "save_path = \"/tmp/LightTunedLoraModel_Final\"  # or your workspace path\n",
        "\n",
        "# List all files and folders in the directory\n",
        "files = os.listdir(save_path)\n",
        "print(files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3a8147",
      "metadata": {
        "id": "bc3a8147",
        "outputId": "a726c853-a79f-49c5-e633-8abe9bb1bc76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned Accuracy: 0.835\n",
            "Base Accuracy: 0.52\n",
            "Accuracy improvement: 0.315\n"
          ]
        }
      ],
      "source": [
        "\n",
        "finetuned_results = trainer.evaluate()\n",
        "print(\"Fine-tuned Accuracy:\", finetuned_results[\"eval_accuracy\"])\n",
        "\n",
        "\n",
        "print(\"Base Accuracy:\", base_results[\"eval_accuracy\"])\n",
        "\n",
        "\n",
        "improvement = finetuned_results[\"eval_accuracy\"] - base_results[\"eval_accuracy\"]\n",
        "print(f\"Accuracy improvement: {improvement:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc96905a",
      "metadata": {
        "id": "bc96905a",
        "outputId": "d5324800-9765-49df-e718-0e938b124e9a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:40]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned Accuracy: 0.787\n",
            "Base Accuracy: 0.471\n",
            "Accuracy improvement: 0.316\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866ab28c",
      "metadata": {
        "id": "866ab28c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a32e4e",
      "metadata": {
        "id": "f9a32e4e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}